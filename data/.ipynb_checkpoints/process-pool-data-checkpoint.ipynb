{
 "metadata": {
  "name": "",
  "signature": "sha256:d674e0b85862ba966be6f883396765deb1bffef1f857ada3df3ad1695a578114"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Render our plots inline\n",
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "\n",
      "pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier\n",
      "plt.rcParams['figure.figsize'] = (15, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load & clean weekly data\n",
      "rawBlocksWeekly = pd.read_csv('weeklyBlockSolves.csv', parse_dates=['Date'])\n",
      "rawBlocksWeekly = rawBlocksWeekly.drop('Unnamed: 0', 1).rename(columns=\n",
      "                                                           {'Date': 'date', \n",
      "                                                            'Pool': 'pool', \n",
      "                                                            'Number of Blocks': 'blocks', \n",
      "                                                            'Percentage of Blocks': 'percentage'})\n",
      "rawBlocksWeekly['period'] = 7\n",
      "rawBlocksWeekly['date-period-midpoint'] = rawBlocksWeekly['date'] - np.timedelta64(4, 'D')\n",
      "\n",
      "# Load & clean daily data\n",
      "rawBlocksDaily = pd.read_csv('dailyBlockSolves.csv', parse_dates=['Date'])\n",
      "rawBlocksDaily = rawBlocksDaily.drop('Unnamed: 0', 1).rename(columns=\n",
      "                                                           {'Date': 'date', \n",
      "                                                            'Pool': 'pool', \n",
      "                                                            'Number of Blocks': 'blocks', \n",
      "                                                            'Percentage of Blocks': 'percentage'})\n",
      "rawBlocksDaily['period'] = 1\n",
      "rawBlocksDaily['date-period-midpoint'] = rawBlocksDaily['date'] - np.timedelta64(1, 'D')\n",
      "\n",
      "# The final object for export begins by building off of the weekly data\n",
      "blocksFrame = rawBlocksWeekly\n",
      "\n",
      "# (OVER)SPECIFIC LITTLE CLEANING ITEMS\n",
      "\n",
      "# Remove dates where percentages total < 99%. \n",
      "# Last I checked there was only one such bad day: 2011-02-27.\n",
      "# But hey, written for generality:\n",
      "datesums = blocksFrame.groupby('date').sum()\n",
      "badDays = datesums[datesums['percentage'] < 99].index.values\n",
      "for badDay in badDays:\n",
      "    blocksFrame = blocksFrame[blocksFrame['date'] != badDay]\n",
      "# That leaves a gap so we reindex.\n",
      "blocksFrame = blocksFrame.reset_index(drop=True)\n",
      "\n",
      "# INTERPOLATION OF DAILY DATA (\"ADAPTIVE RESAMPLING\")\n",
      "    \n",
      "# Find three largest pools\n",
      "topPools = blocksFrame.groupby('pool').max().sort('percentage', ascending=0)  #sort by max percentage\n",
      "topPools = topPools.loc[topPools.index != \"Unknown\"]                          #filter out Unknown\n",
      "topPools = topPools[:3]                                                       #take top 3\n",
      "\n",
      "\"\"\"\n",
      "# For each of the three largest pools...\n",
      "for topPool in topPools.transpose().iteritems():\n",
      "    poolName = topPool[0]\n",
      "    \n",
      "    # find date on which pool hit peak network share (percentage)\n",
      "    poolMaxDate = rawBlocksDaily.iloc[rawBlocksDaily[rawBlocksDaily['pool'] == poolName]['percentage'].idxmax()]['date']\n",
      "    \n",
      "    # find lower bound for high-res region: find all earlier weeklies, and then take the greatest\n",
      "    priorMines = rawBlocksWeekly[(rawBlocksWeekly['pool'] == poolName) & (rawBlocksWeekly['date'] < poolMaxDate)]\n",
      "    priorMinesMaxId = priorMines['date'].idxmax()\n",
      "    priorMinesMax = priorMines.loc[priorMinesMaxId].date\n",
      "    \n",
      "    # find upper bound for high-res region: find all later weeklies, and then take the least\n",
      "    posteriorMines = rawBlocksWeekly[(rawBlocksWeekly['pool'] == poolName) & (rawBlocksWeekly['date'] >= poolMaxDate)]\n",
      "    posteriorMinesMinId = posteriorMines['date'].idxmin()\n",
      "    posteriorMinesMin = posteriorMines.loc[posteriorMinesMinId].date\n",
      "\n",
      "    #todo: remove entries for posteriorMinesMin\n",
      "    \n",
      "    # append daily entries within bounding extent to main dataframe for processing and eventual export \n",
      "    blocksFrame = blocksFrame.append(rawBlocksDaily[(rawBlocksDaily['date'] > priorMinesMax) & (rawBlocksDaily['date'] <= posteriorMinesMin)])\n",
      "\"\"\"\n",
      "\n",
      "# For each of the three largest pools...\n",
      "for topPool in topPools.transpose().iteritems():\n",
      "    poolName = topPool[0]\n",
      "    # find date on which pool hit peak network share (percentage)\n",
      "    poolMaxDate = rawBlocksDaily.iloc[rawBlocksDaily[rawBlocksDaily['pool'] == poolName]['percentage'].idxmax()]['date']\n",
      "    # and append the data for that date\n",
      "    blocksFrame = blocksFrame.append(rawBlocksDaily[rawBlocksDaily['date'] == poolMaxDate])\n",
      "\n",
      "# Append the latest daily data (which is apt to be more recent than the latest weekly)\n",
      "dailyMaxDate = rawBlocksDaily.iloc[rawBlocksDaily['date'].idxmax()]['date']\n",
      "blocksFrame = blocksFrame.append(rawBlocksDaily[rawBlocksDaily['date'] == dailyMaxDate])\n",
      "\n",
      "# I was getting an out-of-bounds error on dateMax below, so I reindex, which seems to work...\n",
      "blocksFrame = blocksFrame.reset_index(drop=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "\n",
      "# Get first and last days of pool block data (will be interval for prices)\n",
      "dateMin = blocksFrame.iloc[blocksFrame['date'].idxmin()]['date'].strftime('%Y-%m-%d')\n",
      "dateMax = blocksFrame.iloc[blocksFrame['date'].idxmax()]['date'].strftime('%Y-%m-%d')\n",
      "\n",
      "# API only supports 2010-07-17 and later.\n",
      "coindeskMin = '2010-07-17'\n",
      "dateMin = dateMin if dateMin > coindeskMin else coindeskMin\n",
      "\n",
      "# Fetching historical prices from Coindesk. API docs at http://www.coindesk.com/api/\n",
      "url = 'http://api.coindesk.com/v1/bpi/historical/close.json?start='+dateMin+'&end='+dateMax\n",
      "response = urllib2.urlopen(url)\n",
      "pricesJSON = response.read()\n",
      "prices = json.loads(pricesJSON)['bpi']\n",
      "\n",
      "datesAndPrices = []\n",
      "for x,y in prices.iteritems():\n",
      "    datesAndPrices.append({'date': x, 'price': y})\n",
      "    \n",
      "datesAndPrices = sorted(datesAndPrices, key=lambda datesAndPrices: datesAndPrices['date'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nest raw blocks \n",
      "pools = blocksFrame.groupby('pool').groups.keys()\n",
      "dates = blocksFrame.groupby('date').groups.keys()\n",
      "\n",
      "# Get aggregate statistics\n",
      "poolMax = blocksFrame.groupby('pool').max()[['percentage']].rename(columns={'percentage':'maxPercentage'})\n",
      "poolSum = blocksFrame.groupby('pool').sum()[['blocks']].rename(columns={'blocks':'sumBlocks'})\n",
      "poolStats = poolMax.join(poolSum)\n",
      "\n",
      "# Build an array of dictionaries fit for our eventual charting purposes\n",
      "blocksByPool = []\n",
      "for name, stats in poolStats.transpose().to_dict().iteritems():\n",
      "    values = []\n",
      "    \n",
      "    for date in dates:\n",
      "        \n",
      "        # numpy date objects will be written to json as %Y-%m-%d\n",
      "        dateString = pd.to_datetime(str(date)).strftime('%Y-%m-%d')\n",
      "        \n",
      "        results = blocksFrame[(blocksFrame['pool'] == name) & (blocksFrame['date'] == date)]\n",
      "        if len(results) == 0:\n",
      "            values.append(\n",
      "                {'date': dateString, \n",
      "                 'percentage': 0, \n",
      "                 'blocks': 0, \n",
      "                 'price': 0,\n",
      "                 'reward': 0,\n",
      "                 'revenue': 0,\n",
      "                 'periodDays': 7 })\n",
      "        else:\n",
      "            \n",
      "            # Cf. https://bitcoinfoundation.org/2012/11/28/happy-halving-day/\n",
      "            # Since this is just working off weekly data, revenue estimates for that week may be off!!!\n",
      "            halvingday = np.datetime64('2012-11-28 15:24:38Z')\n",
      "            reward = 50 if dates[9] < halvingday else 25\n",
      "            \n",
      "            # Fetch Bitcoin Coindesk Price Index (BPI) for date. Cf. www.coindesk.com/api/\n",
      "            price = prices[dateString] if dateString in prices else 0\n",
      "            \n",
      "            values.append(\n",
      "                {'date': dateString, \n",
      "                 'percentage': results[['percentage']].values[0][0]/100, \n",
      "                 'blocks': results[['blocks']].values[0][0],\n",
      "                 'price': price,\n",
      "                 'reward': reward,\n",
      "                 'revenue': results[['blocks']].values[0][0] * price * reward,\n",
      "                 'periodDays': 7 })\n",
      "    \n",
      "    blocksByPool.append(\n",
      "        {'name': name, \n",
      "         'maxPercentage': stats['maxPercentage']/100, \n",
      "         'sumBlocks': stats['sumBlocks'],\n",
      "         'values': values})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {'pools': blocksByPool,\n",
      "        'dates': datesAndPrices,\n",
      "        'sources': \n",
      "            [{ 'name': 'Organ of Corti',\n",
      "               'data': 'Block mining data',\n",
      "               'url': 'http://organofcorti.blogspot.com/',\n",
      "               'file': False,\n",
      "               'fileType': False },\n",
      "             { 'name': 'Coindesk',\n",
      "               'data': 'Bitcoin price data',\n",
      "               'url': 'http://coindesk.com/api',\n",
      "               'file': url,\n",
      "               'fileType': 'json' }],\n",
      "        'colophon': \"Data is derived from claims made by miners, either on their websites or in coinbase signatures, and known addresses. Due to the anonymous nature of the Bitcoin network, mining (in the form of block solves) is only identifiable if declared, and even then is not verifiable. Notably, Slush's pool was the first in operation (in December 2010), but data for it only became available in January 2012. Data is weekly except for the three weeks when DeepBit, BTC Guild, and GHash.IO peaked, during which daily data is shown; thus, those jagged spans indicate higher-resolution data, not necessarily higher intrinsic volatility. Inferred network share percentages are subject to fluctuations due to luck; e.g., over a ten-minute period, there may be only one miner to successfully mine a block, but one should not infer an instantaneous network share of 100%. Furthermore, there is no guarantee that your sense perceptions correspond to an objective reality, or indeed that a world exists outside your head.\",\n",
      "        'credits': {'author': 'Toph Tucker'}\n",
      "        }\n",
      "\n",
      "with open('data.json', 'w') as outfile:\n",
      "  json.dump(data, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}