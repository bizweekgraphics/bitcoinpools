{
 "metadata": {
  "name": "",
  "signature": "sha256:8043ed2e53cc9db3bc00dd2e186dc359b98a4312d62633dc717530301e54e8bc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Render our plots inline\n",
      "%matplotlib inline\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "\n",
      "pd.set_option('display.mpl_style', 'default') # Make the graphs a bit prettier\n",
      "plt.rcParams['figure.figsize'] = (15, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 486
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rawBlocksWeekly = pd.read_csv('weeklyBlockSolves.csv', parse_dates=['Date'])\n",
      "rawBlocksDaily = pd.read_csv('dailyBlockSolves.csv', parse_dates=['Date'])\n",
      "\n",
      "# Todo: interpolate weekly and daily (\"adaptive resampling\")\n",
      "blocksFrame = rawBlocksDaily.drop('Unnamed: 0', 1).rename(columns=\n",
      "                                                           {'Date': 'date', \n",
      "                                                            'Pool': 'pool', \n",
      "                                                            'Number of Blocks': 'blocks', \n",
      "                                                            'Percentage of Blocks': 'percentage'})\n",
      "\n",
      "# (OVER)SPECIFIC LITTLE CLEANING ITEMS\n",
      "\n",
      "# Remove dates where percentages total < 99%. \n",
      "# Last I checked there was only one such bad day: 2011-02-27.\n",
      "# But hey, written for generality:\n",
      "datesums = blocksFrame.groupby('date').sum()\n",
      "badDays = datesums[datesums['percentage'] < 99].index.values\n",
      "for badDay in badDays:\n",
      "    blocksFrame = blocksFrame[blocksFrame['date'] != badDay]\n",
      "# That leaves a gap so we reindex.\n",
      "blocksFrame = blocksFrame.reset_index(drop=True)\n",
      "\n",
      "# INTERPOLATION OF DAILY DATA (\"ADAPTIVE RESAMPLING\")\n",
      "    \n",
      "# Find largest pools\n",
      "topPools = blocksFrame.groupby('pool').max().sort('percentage', ascending=0)  #sort by max percentage\n",
      "topPools = topPools.loc[topPools.index != \"Unknown\"]                          #filter out Unknown\n",
      "topPools = topPools[:3]                                                       #take top 3\n",
      "\n",
      "\"\"\"\n",
      "Todo:\n",
      "- [x] Find largest pools\n",
      "- [x] Find day on which each maxed out\n",
      "- [x] For each max day...\n",
      "- [ ] Find all earlier weeklies, take greatest\n",
      "- [ ] Find all later weeklies, take least (& delete)\n",
      "- [ ] Append all daily data from dates > former & <= latter\n",
      "\"\"\"\n",
      "\n",
      "poolMaxes = []\n",
      "for topPool in topPools.transpose().iteritems():\n",
      "    poolMaxes.append(\n",
      "        {'pool': topPool[0], \n",
      "         'date': blocksFrame.iloc[blocksFrame[blocksFrame['pool'] == topPool[0]]['percentage'].idxmax()]['date']})\n",
      "\n",
      "#rawBlocksDaily.groupby('Pool').max().sort('Percentage of Blocks', ascending=0)\n",
      "\n",
      "#rawBlocksDaily.iloc[rawBlocksDaily[rawBlocksDaily['Pool'] == 'BTC Guild']['Percentage of Blocks'].idxmax()]['Date']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 495
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "\n",
      "# Get first and last days of pool block data (will be interval for prices)\n",
      "dateMin = blocksFrame.iloc[blocksFrame['date'].idxmin()]['date'].strftime('%Y-%m-%d')\n",
      "dateMax = blocksFrame.iloc[blocksFrame['date'].idxmax()]['date'].strftime('%Y-%m-%d')\n",
      "\n",
      "# API only supports 2010-07-17 and later.\n",
      "coindeskMin = '2010-07-17'\n",
      "dateMin = dateMin if dateMin > coindeskMin else coindeskMin\n",
      "\n",
      "# Fetching historical prices from Coindesk. API docs at http://www.coindesk.com/api/\n",
      "url = 'http://api.coindesk.com/v1/bpi/historical/close.json?start='+dateMin+'&end='+dateMax\n",
      "response = urllib2.urlopen(url)\n",
      "pricesJSON = response.read()\n",
      "prices = json.loads(pricesJSON)['bpi']\n",
      "\n",
      "datesAndPrices = []\n",
      "for x,y in prices.iteritems():\n",
      "    datesAndPrices.append({'date': x, 'price': y})\n",
      "    \n",
      "datesAndPrices = sorted(datesAndPrices, key=lambda datesAndPrices: datesAndPrices['date'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 521
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nest raw blocks \n",
      "pools = blocksFrame.groupby('pool').groups.keys()\n",
      "dates = blocksFrame.groupby('date').groups.keys()\n",
      "\n",
      "# Get aggregate statistics\n",
      "poolMax = blocksFrame.groupby('pool').max()[['percentage']].rename(columns={'percentage':'maxPercentage'})\n",
      "poolSum = blocksFrame.groupby('pool').sum()[['blocks']].rename(columns={'blocks':'sumBlocks'})\n",
      "poolStats = poolMax.join(poolSum)\n",
      "\n",
      "# Build an array of dictionaries fit for our eventual charting purposes\n",
      "blocksByPool = []\n",
      "for name, stats in poolStats.transpose().to_dict().iteritems():\n",
      "    values = []\n",
      "    \n",
      "    for date in dates:\n",
      "        \n",
      "        # numpy date objects will be written to json as %Y-%m-%d\n",
      "        dateString = pd.to_datetime(str(date)).strftime('%Y-%m-%d')\n",
      "        \n",
      "        results = blocksFrame[(blocksFrame['pool'] == name) & (blocksFrame['date'] == date)]\n",
      "        if len(results) == 0:\n",
      "            values.append(\n",
      "                {'date': dateString, \n",
      "                 'percentage': 0, \n",
      "                 'blocks': 0, \n",
      "                 'price': 0,\n",
      "                 'reward': 0,\n",
      "                 'revenue': 0,\n",
      "                 'periodDays': 7 })\n",
      "        else:\n",
      "            \n",
      "            # Cf. https://bitcoinfoundation.org/2012/11/28/happy-halving-day/\n",
      "            # Since this is just working off weekly data, revenue estimates for that week may be off!!!\n",
      "            halvingday = np.datetime64('2012-11-28 15:24:38Z')\n",
      "            reward = 50 if dates[9] < halvingday else 25\n",
      "            \n",
      "            # Fetch Bitcoin Coindesk Price Index (BPI) for date. Cf. www.coindesk.com/api/\n",
      "            price = prices[dateString]\n",
      "            \n",
      "            values.append(\n",
      "                {'date': dateString, \n",
      "                 'percentage': results[['percentage']].values[0][0]/100, \n",
      "                 'blocks': results[['blocks']].values[0][0],\n",
      "                 'price': price,\n",
      "                 'reward': reward,\n",
      "                 'revenue': results[['blocks']].values[0][0] * price * reward,\n",
      "                 'periodDays': 7 })\n",
      "    \n",
      "    blocksByPool.append(\n",
      "        {'name': name, \n",
      "         'maxPercentage': stats['maxPercentage']/100, \n",
      "         'sumBlocks': stats['sumBlocks'],\n",
      "         'values': values})\n",
      "\n",
      "blocksByPool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'2009-01-10'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-522-22b5e4483e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Fetch Bitcoin Coindesk Price Index (BPI) for date. Cf. www.coindesk.com/api/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdateString\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             values.append(\n",
        "\u001b[0;31mKeyError\u001b[0m: '2009-01-10'"
       ]
      }
     ],
     "prompt_number": 522
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = {'pools': blocksByPool,\n",
      "        'dates': datesAndPrices,\n",
      "        'sources': \n",
      "            [{ 'name': 'Organ of Corti',\n",
      "               'data': 'Block mining data',\n",
      "               'url': 'http://organofcorti.blogspot.com/',\n",
      "               'file': False,\n",
      "               'fileType': False },\n",
      "             { 'name': 'Coindesk',\n",
      "               'data': 'Bitcoin price data',\n",
      "               'url': 'http://coindesk.com/api',\n",
      "               'file': url,\n",
      "               'fileType': 'json' }],\n",
      "        'colophon': \"Data is derived from claims made by miners, either on their websites or in coinbase signatures, and known addresses. Due to the anonymous nature of the Bitcoin network, mining (in the form of block solves) is only identifiable if declared, and even then is not verifiable. Notably, Slush's pool was the first in operation (in December 2010), but data for it only became available in January 2012. Data is weekly except for the three weeks when DeepBit, BTC Guild, and GHash.IO peaked, during which daily data is shown; thus, those jagged spans indicate higher-resolution data, not necessarily higher intrinsic volatility. Inferred network share percentages are subject to fluctuations due to luck; e.g., over a ten-minute period, there may be only one miner to successfully mine a block, but one should not infer an instantaneous network share of 100%. Furthermore, there is no guarantee that your sense perceptions correspond to an objective reality, or indeed that a world exists outside your head.\",\n",
      "        'credits': {'author': 'Toph Tucker'}\n",
      "        }\n",
      "\n",
      "with open('data.json', 'w') as outfile:\n",
      "  json.dump(data, outfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 417
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}